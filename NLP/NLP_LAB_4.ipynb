{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab4_text_classification(Completed).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 4 - Neural Network Classifier Using Simple Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to solve a text classification task using simple \n",
        "feedforward neural network classifier. We will use IMDB Large Movie Review Dataset to train a binary classification model, able to predict whether a review is positive or negative. First, our network takes one-hot word vectors as input, averages them to make one vector and trains a \n",
        "fully-connected layer to predict the output. In the second part, we replace the one-hot vectors with the word embeddings and add a layer to see how much that improves the performance.\n",
        "\n",
        "We are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. But it is not straightforward to define models where layers connect to more than just the previous and next layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8fpBfhBpupy",
        "outputId": "e5e7e5d1-ed9b-40f0-9565-7801d21dfb9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EundMtGPpCdf"
      },
      "source": [
        "The dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 50000 labeled movie reviews. These are split into 25,000 reviews for training and 25,000 reviews for testing. The  dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The data is preprocessed. For text classification, it is ususal to limit the size of the vocabulary to stop the dataset from becoming too sparse, creating possible overfitting. We keep the top 10,000 most frequently occurring words in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NyuSzkafqNca",
        "colab": {}
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see the length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-gjWRAuqg5s",
        "outputId": "e63841c3-223f-4023-d2aa-1aa7d49b7ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(X_train), len(y_train)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "The  reviews have been converted to integers and each integer represents a  word in a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "79Ev72Kgq4XL",
        "outputId": "e2b3cf8b-9c1f-4ea2-8925-cc20d0cd11f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " X_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "We can convert integers back to words by querying a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMCH1OoDrSNR",
        "colab": {}
      },
      "source": [
        "\n",
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5IreFXgruZot"
      },
      "source": [
        "Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown tokens. Index 0 will be used for padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abIb7Fe5u3GQ",
        "colab": {}
      },
      "source": [
        "\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<UNUSED>\"] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "To reverse key and values in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKOiVVXQu-_I",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZmTJEm8xvUvW"
      },
      "source": [
        "To view a word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SqN5jgVKvJJZ",
        "outputId": "bc50a40a-83a3-41c2-c217-364bb6be8c5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reverse_word_index[25]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6QjrzgVvrYn"
      },
      "source": [
        "And to recreate the whole sentence from our training data we define decode_review:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvrKeMgxvWlv",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sxg4YA_NvdRg",
        "outputId": "6a1204cd-63d0-4b87-b1c5-7b351d666bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "decode_review(X_train[10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c8gIzXncfaJK"
      },
      "source": [
        "### Creating One-hot word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9W4yb3rv_E0"
      },
      "source": [
        "It is  common to use one-hot representation as input in Natural Language Processing tasks. In Keras, the Embedding layer takes an index as an input and convert it to one-hot vector with the length of the vocabulary size. Then multiplies these vectors by a normal weight matrix. But there is no way to only get a one-hot vector as the output of a layer in Keras. To solve this we use Lambda() layer and a function that creates the one-hot layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RPO_pK9zH4C5",
        "colab": {}
      },
      "source": [
        "def OneHot(input_dim=None, input_length=None):\n",
        "    \n",
        "    if input_dim is None or input_length is None:\n",
        "        raise TypeError(\"input_dim or input_length is not set\")\n",
        "\n",
        "    \n",
        "    def _one_hot(x, num_classes):\n",
        "        return K.one_hot(K.cast(x, 'uint8'),\n",
        "                          num_classes=num_classes)\n",
        "\n",
        "    return Lambda(_one_hot,\n",
        "                  arguments={'num_classes': input_dim},\n",
        "                  input_shape=(input_length,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "364d3MAw0ez9"
      },
      "source": [
        "input_dim refers to the length of the one-hot vector and input_length refers to the length of the input sequence. Since the input to K.one_hot should be an integer tensor, we cast x to one (Keras passes around float tensors by default).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VHz76GNA2M4r"
      },
      "source": [
        " Each text sequence has in most cases different length of words. Here, we fill sequences with a pad token (0) to fit the size. This special tokens is then masked not to be accounted in averaging, loss calculation etc. We set the maximum length to 256."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9G_o7PsvgSFt"
      },
      "source": [
        "### Preparing input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jiFn7sd_wF5j",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "X_train_enc = keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "X_test_enc = keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kcjFH1wKF_7d"
      },
      "source": [
        "And to view a padded review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zwH4dcfW_a18",
        "outputId": "38a743d6-8cdc-4383-b8c7-da425ae536fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "print(X_train_enc[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
            "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
            "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
            "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
            "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
            "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
            "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
            "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
            " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
            "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
            "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
            " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
            "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
            "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1zcxFwNGepA"
      },
      "source": [
        "Now we want to build the neural network model. We  are going to have a hidden layer with 16 hidden units. \n",
        "\n",
        "First, we want to transform each index to an embedded vector and then average all vectors to a single one. It has been showed that unweighted average of word vectors outperforms many complicated networks that model semantic and syntactic compositionality. As an example you can take a look at this: (http://anthology.aclweb.org/P/P15/P15-1162.pdf)\n",
        "\n",
        "To average we need to ignore padded zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yi04MLIvJOGZ",
        "colab": {}
      },
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "whgIIB5ggjna"
      },
      "source": [
        "### Neural Network model using one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jlOLnlnSJgrU"
      },
      "source": [
        "The first layer is an one-hot layer. The second layer is to compute average on all word vectors in a sentence without considering padding. The  output vector is piped through a fully-connected layer. The last layer is connected with a single output node with the sigmoid activation function. The final value is a float between 0 and 1. \n",
        "The vocabulary count of the movie reviews (10000) is used as the input shape. At the end we visualize the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Pn83gBbxiK7",
        "outputId": "672074db-6cf0-4141-9173-abf70f5d773f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# put your code here\n",
        "model = Sequential()\n",
        "model.add(OneHot(input_dim=VOCAB_SIZE,input_length=MAX_SEQUENCE_LENGTH))  #one hot layer\n",
        "model.add(GlobalAveragePooling1DMasked())   #average of word vector\n",
        "model.add(Dense(units=16))                  #fully conected layer with 16 units\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Mz96xpCgvTj"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F3HbW_IKLqwT"
      },
      "source": [
        "To compile the model we need a loss function and an optimizer. We use binary_crossentropy loss function which is just a special case of categorical cross entropy. We also use Adam optimizer that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can read more about it here:\n",
        "(https://arxiv.org/abs/1412.6980v8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qh1PWTNMxjUw",
        "outputId": "9881ac78-2e1f-4174-8d84-a93a756556b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1jwQQqCN5Ia"
      },
      "source": [
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5lAqzQlxjSM",
        "colab": {}
      },
      "source": [
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "y_val = np.array(y_train[:10000])\n",
        "partial_y_train = np.array(y_train[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E8Kpo5G3OJEY"
      },
      "source": [
        "Then we start to train the model for 40 epochs in mini-batches of 512 samples and monitor the model's loss and accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "99_z39KAxjPi",
        "outputId": "9056f8fa-489f-4d89-942d-8cd0bd377ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "15000/15000 [==============================] - 16s 1ms/step - loss: 0.6921 - acc: 0.5441 - val_loss: 0.6911 - val_acc: 0.4991\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 6s 423us/step - loss: 0.6895 - acc: 0.5655 - val_loss: 0.6883 - val_acc: 0.6554\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 6s 423us/step - loss: 0.6865 - acc: 0.6395 - val_loss: 0.6852 - val_acc: 0.6579\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 6s 423us/step - loss: 0.6827 - acc: 0.6617 - val_loss: 0.6814 - val_acc: 0.6632\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 6s 419us/step - loss: 0.6783 - acc: 0.6745 - val_loss: 0.6768 - val_acc: 0.6700\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 6s 419us/step - loss: 0.6734 - acc: 0.6779 - val_loss: 0.6722 - val_acc: 0.6710\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 6s 419us/step - loss: 0.6679 - acc: 0.6827 - val_loss: 0.6670 - val_acc: 0.6726\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 6s 420us/step - loss: 0.6619 - acc: 0.6818 - val_loss: 0.6609 - val_acc: 0.6718\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 6s 419us/step - loss: 0.6560 - acc: 0.6858 - val_loss: 0.6552 - val_acc: 0.6831\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 6s 421us/step - loss: 0.6497 - acc: 0.6898 - val_loss: 0.6492 - val_acc: 0.6802\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 6s 422us/step - loss: 0.6434 - acc: 0.6923 - val_loss: 0.6438 - val_acc: 0.6884\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 6s 426us/step - loss: 0.6370 - acc: 0.6956 - val_loss: 0.6381 - val_acc: 0.6907\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 6s 428us/step - loss: 0.6310 - acc: 0.6968 - val_loss: 0.6324 - val_acc: 0.6925\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 6s 426us/step - loss: 0.6247 - acc: 0.6998 - val_loss: 0.6260 - val_acc: 0.6987\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 6s 426us/step - loss: 0.6184 - acc: 0.7027 - val_loss: 0.6204 - val_acc: 0.7018\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 6s 429us/step - loss: 0.6127 - acc: 0.7068 - val_loss: 0.6154 - val_acc: 0.7036\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 6s 430us/step - loss: 0.6070 - acc: 0.7085 - val_loss: 0.6097 - val_acc: 0.7084\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 6s 430us/step - loss: 0.6015 - acc: 0.7119 - val_loss: 0.6042 - val_acc: 0.7114\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 6s 429us/step - loss: 0.5958 - acc: 0.7157 - val_loss: 0.5991 - val_acc: 0.7130\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 6s 431us/step - loss: 0.5906 - acc: 0.7193 - val_loss: 0.5945 - val_acc: 0.7182\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 6s 431us/step - loss: 0.5856 - acc: 0.7230 - val_loss: 0.5894 - val_acc: 0.7179\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 6s 431us/step - loss: 0.5810 - acc: 0.7211 - val_loss: 0.5853 - val_acc: 0.7225\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 7s 434us/step - loss: 0.5761 - acc: 0.7267 - val_loss: 0.5813 - val_acc: 0.7228\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 6s 431us/step - loss: 0.5718 - acc: 0.7261 - val_loss: 0.5764 - val_acc: 0.7246\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 7s 434us/step - loss: 0.5675 - acc: 0.7283 - val_loss: 0.5727 - val_acc: 0.7225\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 7s 434us/step - loss: 0.5636 - acc: 0.7318 - val_loss: 0.5687 - val_acc: 0.7269\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 7s 434us/step - loss: 0.5596 - acc: 0.7331 - val_loss: 0.5651 - val_acc: 0.7287\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 7s 437us/step - loss: 0.5560 - acc: 0.7371 - val_loss: 0.5617 - val_acc: 0.7307\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 7s 436us/step - loss: 0.5526 - acc: 0.7381 - val_loss: 0.5585 - val_acc: 0.7324\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 7s 438us/step - loss: 0.5496 - acc: 0.7395 - val_loss: 0.5556 - val_acc: 0.7332\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 7s 438us/step - loss: 0.5464 - acc: 0.7420 - val_loss: 0.5527 - val_acc: 0.7370\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 7s 437us/step - loss: 0.5437 - acc: 0.7431 - val_loss: 0.5505 - val_acc: 0.7343\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 7s 440us/step - loss: 0.5412 - acc: 0.7421 - val_loss: 0.5483 - val_acc: 0.7370\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 7s 441us/step - loss: 0.5384 - acc: 0.7457 - val_loss: 0.5452 - val_acc: 0.7403\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 7s 439us/step - loss: 0.5364 - acc: 0.7466 - val_loss: 0.5431 - val_acc: 0.7413\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 7s 444us/step - loss: 0.5339 - acc: 0.7452 - val_loss: 0.5411 - val_acc: 0.7425\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 7s 443us/step - loss: 0.5320 - acc: 0.7480 - val_loss: 0.5396 - val_acc: 0.7401\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 7s 441us/step - loss: 0.5298 - acc: 0.7481 - val_loss: 0.5374 - val_acc: 0.7445\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 7s 443us/step - loss: 0.5281 - acc: 0.7489 - val_loss: 0.5360 - val_acc: 0.7446\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 7s 445us/step - loss: 0.5270 - acc: 0.7487 - val_loss: 0.5343 - val_acc: 0.7447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i_9a_rybhG5J"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYLH8kOgOo9W"
      },
      "source": [
        "To evaulate the model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFMt2Q7b3taP",
        "outputId": "a0f6ca47-17fa-4f5f-f28d-5c8b6340ee32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 7s 297us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RrKiPHcAmQU",
        "outputId": "2d05051f-97d2-43f0-aa90-e4c352a6e132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(results)\n",
        "# loss, accuracay "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5341577560806274, 0.74236]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrWNtROhusRg",
        "colab_type": "code",
        "outputId": "be5cf3c4-c677-4af3-a90b-73c1aac6ef9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#sanity check\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 256, 10000)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pW7IpHxMO6qp"
      },
      "source": [
        "Our first model accuracy using one-hot vectors is 0.742 (74%)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwZk_yoWhPJB"
      },
      "source": [
        "### Plotting the accuracy graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIDPH1J7PMzN"
      },
      "source": [
        "To plot a graph of accuracy and loss over time we can use Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LS9k2vvSAqB7",
        "outputId": "4bb18f5d-c93c-4ea6-b66d-2a444436bc96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJJKUGIiEhS/w+9wV471b5X6QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "### Neural Network model using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now instead of one-hot vectors, we want to use embedding. We change our first layer in model1 to an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFrCsL-NBFVL",
        "outputId": "c77b2947-351f-4cd9-b56a-f887b7160472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "VOCAB_SIZE= 10000\n",
        "\n",
        "# put the code here\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(VOCAB_SIZE,100))  #Embedding layer \n",
        "model2.add(GlobalAveragePooling1DMasked())   #average of word vector\n",
        "model2.add(Dense(units=16))                  #fully conected layer with 16 units\n",
        "model2.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history2 = model2.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model2.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 1s 42us/step - loss: 0.6880 - acc: 0.6141 - val_loss: 0.6804 - val_acc: 0.6448\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.6616 - acc: 0.7122 - val_loss: 0.6394 - val_acc: 0.7414\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.5993 - acc: 0.7691 - val_loss: 0.5678 - val_acc: 0.7740\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.5123 - acc: 0.8093 - val_loss: 0.4844 - val_acc: 0.8100\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.4248 - acc: 0.8485 - val_loss: 0.4139 - val_acc: 0.8438\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.3566 - acc: 0.8745 - val_loss: 0.3685 - val_acc: 0.8580\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.3092 - acc: 0.8897 - val_loss: 0.3381 - val_acc: 0.8678\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.2729 - acc: 0.9029 - val_loss: 0.3194 - val_acc: 0.8731\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.2449 - acc: 0.9131 - val_loss: 0.3057 - val_acc: 0.8776\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.2219 - acc: 0.9215 - val_loss: 0.2973 - val_acc: 0.8814\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.2019 - acc: 0.9300 - val_loss: 0.2919 - val_acc: 0.8837\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1840 - acc: 0.9377 - val_loss: 0.2884 - val_acc: 0.8840\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.1685 - acc: 0.9443 - val_loss: 0.2882 - val_acc: 0.8848\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1551 - acc: 0.9502 - val_loss: 0.2890 - val_acc: 0.8842\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1437 - acc: 0.9531 - val_loss: 0.2959 - val_acc: 0.8808\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1319 - acc: 0.9595 - val_loss: 0.3020 - val_acc: 0.8794\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.1214 - acc: 0.9639 - val_loss: 0.2996 - val_acc: 0.8849\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1112 - acc: 0.9687 - val_loss: 0.3054 - val_acc: 0.8839\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1025 - acc: 0.9712 - val_loss: 0.3118 - val_acc: 0.8829\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 24us/step - loss: 0.0946 - acc: 0.9743 - val_loss: 0.3189 - val_acc: 0.8820\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 24us/step - loss: 0.0869 - acc: 0.9773 - val_loss: 0.3274 - val_acc: 0.8803\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 25us/step - loss: 0.0800 - acc: 0.9809 - val_loss: 0.3356 - val_acc: 0.8794\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0736 - acc: 0.9823 - val_loss: 0.3468 - val_acc: 0.8787\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0682 - acc: 0.9839 - val_loss: 0.3550 - val_acc: 0.8770\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0629 - acc: 0.9862 - val_loss: 0.3647 - val_acc: 0.8771\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0576 - acc: 0.9885 - val_loss: 0.3759 - val_acc: 0.8769\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0530 - acc: 0.9891 - val_loss: 0.3877 - val_acc: 0.8761\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0492 - acc: 0.9907 - val_loss: 0.3986 - val_acc: 0.8747\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 24us/step - loss: 0.0450 - acc: 0.9921 - val_loss: 0.4104 - val_acc: 0.8740\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0413 - acc: 0.9925 - val_loss: 0.4253 - val_acc: 0.8715\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0393 - acc: 0.9934 - val_loss: 0.4384 - val_acc: 0.8695\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0353 - acc: 0.9945 - val_loss: 0.4478 - val_acc: 0.8701\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0322 - acc: 0.9957 - val_loss: 0.4590 - val_acc: 0.8706\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0297 - acc: 0.9962 - val_loss: 0.4723 - val_acc: 0.8684\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0273 - acc: 0.9969 - val_loss: 0.4859 - val_acc: 0.8673\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0256 - acc: 0.9968 - val_loss: 0.4986 - val_acc: 0.8678\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0233 - acc: 0.9973 - val_loss: 0.5099 - val_acc: 0.8669\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 23us/step - loss: 0.0214 - acc: 0.9978 - val_loss: 0.5181 - val_acc: 0.8659\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0194 - acc: 0.9979 - val_loss: 0.5301 - val_acc: 0.8662\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.0181 - acc: 0.9985 - val_loss: 0.5422 - val_acc: 0.8658\n",
            "25000/25000 [==============================] - 1s 39us/step\n",
            "[0.5766747018527985, 0.85488]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I4zIPJDcTPq3",
        "outputId": "ed712206-4d78-4399-c5f1-a25e3ef5720b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model2.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 39us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "waS96edDTRyL",
        "outputId": "9cd3eb99-5b7b-4933-8d7a-fdfa29686f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5766747018527985, 0.85488]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRouY7EStzEi",
        "colab_type": "code",
        "outputId": "091a6676-2bed-4adf-e7d5-84ceb18c0352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                1616      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,001,633\n",
            "Trainable params: 1,001,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XB7aveVzTC5a",
        "outputId": "5d7593d9-0ec6-44a1-b2b4-632f48346f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history2.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of model2 is 87%. Using Embedding layer instead of one-hot layer improved the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "--020hfG6rN2"
      },
      "source": [
        "### Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_PypdqG9Iis",
        "colab": {}
      },
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gembn7VM3ex8",
        "colab": {}
      },
      "source": [
        "from keras.initializers import Constant\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwjuuS42rDF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enWTwkperEPQ",
        "colab_type": "code",
        "outputId": "a12c4b33-ef64-4342-b0f9-a21580a7b019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZCPUM0W_Drc",
        "colab": {}
      },
      "source": [
        "# put the code here\n",
        "wordToIndex, indexToWord, wordToGlove = readGloveFile('drive/My Drive/Colab Notebooks/glove.6B.300d.txt')\n",
        "model3 = Sequential()\n",
        "model3.add(createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,False)) #setting isTrainable to true \n",
        "model3.add(GlobalAveragePooling1DMasked())   #average of word vector\n",
        "model3.add(Dense(units=16))                  #fully conected layer with 16 units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "### Adding another hidden layer to the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbZ6UBDfbjea"
      },
      "source": [
        "In model3, we only add another dense layer to see if that improves the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vw0le1YjDdCa",
        "outputId": "4ebba8e8-d00c-42a2-e995-70aa8e35c2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# put your code here\n",
        "model3.add(Dense(16))   #adding extra layer \n",
        "model3.add(Dense(1,activation='sigmoid')) #output layer\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history3 = model3.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model3.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:421: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120000300 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 10s 653us/step - loss: 0.6837 - acc: 0.5689 - val_loss: 0.6681 - val_acc: 0.6229\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.6378 - acc: 0.6712 - val_loss: 0.6015 - val_acc: 0.7008\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.5339 - acc: 0.7568 - val_loss: 0.4771 - val_acc: 0.7947\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.4085 - acc: 0.8345 - val_loss: 0.3834 - val_acc: 0.8435\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.3194 - acc: 0.8736 - val_loss: 0.3374 - val_acc: 0.8632\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.2632 - acc: 0.8974 - val_loss: 0.3170 - val_acc: 0.8709\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.2214 - acc: 0.9155 - val_loss: 0.3002 - val_acc: 0.8811\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.1899 - acc: 0.9294 - val_loss: 0.3013 - val_acc: 0.8819\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.1621 - acc: 0.9430 - val_loss: 0.3015 - val_acc: 0.8835\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.1395 - acc: 0.9529 - val_loss: 0.3113 - val_acc: 0.8833\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.1191 - acc: 0.9629 - val_loss: 0.3234 - val_acc: 0.8829\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.1003 - acc: 0.9699 - val_loss: 0.3448 - val_acc: 0.8792\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0864 - acc: 0.9729 - val_loss: 0.3629 - val_acc: 0.8789\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0725 - acc: 0.9799 - val_loss: 0.3862 - val_acc: 0.8771\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0622 - acc: 0.9838 - val_loss: 0.4261 - val_acc: 0.8689\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0518 - acc: 0.9889 - val_loss: 0.4462 - val_acc: 0.8726\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0422 - acc: 0.9918 - val_loss: 0.4720 - val_acc: 0.8721\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0354 - acc: 0.9940 - val_loss: 0.5031 - val_acc: 0.8696\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0298 - acc: 0.9949 - val_loss: 0.5376 - val_acc: 0.8695\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.0251 - acc: 0.9959 - val_loss: 0.5684 - val_acc: 0.8665\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0193 - acc: 0.9973 - val_loss: 0.6026 - val_acc: 0.8653\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0171 - acc: 0.9978 - val_loss: 0.6267 - val_acc: 0.8653\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0132 - acc: 0.9985 - val_loss: 0.6623 - val_acc: 0.8635\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0109 - acc: 0.9991 - val_loss: 0.6843 - val_acc: 0.8638\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.0091 - acc: 0.9993 - val_loss: 0.7090 - val_acc: 0.8639\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 5s 309us/step - loss: 0.0075 - acc: 0.9996 - val_loss: 0.7374 - val_acc: 0.8626\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 5s 309us/step - loss: 0.0064 - acc: 0.9996 - val_loss: 0.7555 - val_acc: 0.8628\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0054 - acc: 0.9997 - val_loss: 0.7756 - val_acc: 0.8629\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0047 - acc: 0.9998 - val_loss: 0.7951 - val_acc: 0.8626\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0040 - acc: 0.9999 - val_loss: 0.8136 - val_acc: 0.8624\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0034 - acc: 0.9999 - val_loss: 0.8300 - val_acc: 0.8616\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.8479 - val_acc: 0.8613\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.8617 - val_acc: 0.8617\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 5s 308us/step - loss: 0.0025 - acc: 0.9999 - val_loss: 0.8734 - val_acc: 0.8612\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.8861 - val_acc: 0.8614\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0020 - acc: 0.9999 - val_loss: 0.8985 - val_acc: 0.8608\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.9119 - val_acc: 0.8608\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 5s 306us/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.9241 - val_acc: 0.8598\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.9324 - val_acc: 0.8611\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 5s 307us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9452 - val_acc: 0.8594\n",
            "25000/25000 [==============================] - 1s 40us/step\n",
            "[1.0132289003276824, 0.8454]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PRUBOcWtcWc",
        "colab_type": "code",
        "outputId": "7e9a9618-463d-4a38-ff77-1d96cfea92cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model3.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 300)         120000300 \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 120,005,405\n",
            "Trainable params: 120,005,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QtsdVeW7UgCu",
        "outputId": "94078993-1453-492c-fecf-8b3682479752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history3.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU9b3/8deHNYQdwqJECCoVQQQh\nIv7EvfbiBr8qVZFad5SKW7VXKtatYG9t63W5/LzGVqsVQa5Wq63LVaSitSBBSNgUEAMGEAEBCaAQ\n+P7++J4hkzCTTJbJmWTez8fjPOZsc+YzB3I+c77bMeccIiKSvpqEHYCIiIRLiUBEJM0pEYiIpDkl\nAhGRNKdEICKS5pQIRETSnBKBHMTMmppZiZn1rMt9w2RmR5pZnbeVNrPvm1lR1PKnZnZyIvvW4LP+\nYGZ31vT9IvE0CzsAqT0zK4lazAS+A/YFy9c556ZV53jOuX1Am7reNx04546qi+OY2TXAj51zp0Ud\n+5q6OLZIRUoEjYBz7sCFOPjFeY1z7p14+5tZM+dcaX3EJlIV/X8Mn4qG0oCZTTazF8xsupntAH5s\nZiea2Vwz22ZmG8zsUTNrHuzfzMycmeUEy88F298wsx1m9i8z613dfYPtZ5vZCjPbbmaPmdk/zeyK\nOHEnEuN1ZrbKzLaa2aNR721qZv9pZlvMbDUwopLzM8nMZlRYN9XMHgrmrzGz5cH3+Sz4tR7vWMVm\ndlown2lmfw5iWwoMqbDvXWa2OjjuUjMbGawfAPwXcHJQ7LY56tzeG/X+64PvvsXMXjGzQxI5N9U5\nz5F4zOwdM/vazL40s3+P+pxfBufkGzPLN7NDYxXDmdkHkX/n4HzOCT7na+AuM+tjZrODz9gcnLf2\nUe/vFXzHTcH2R8wsI4j56Kj9DjGzXWbWOd73lRicc5oa0QQUAd+vsG4ysAc4H5/8WwHHAyfg7woP\nB1YAE4L9mwEOyAmWnwM2A7lAc+AF4Lka7NsV2AGMCrb9DNgLXBHnuyQS41+B9kAO8HXkuwMTgKVA\nNtAZmOP/u8f8nMOBEqB11LG/AnKD5fODfQw4A9gNHBts+z5QFHWsYuC0YP53wD+AjkAvYFmFfS8C\nDgn+TS4NYugWbLsG+EeFOJ8D7g3mfxDEOAjIAP4f8G4i56aa57k9sBG4GWgJtAOGBtt+ARQAfYLv\nMAjoBBxZ8VwDH0T+nYPvVgqMB5ri/z9+DzgTaBH8P/kn8Luo77MkOJ+tg/1PCrblAVOiPuc24OWw\n/w4b2hR6AJrq+B80fiJ4t4r33Q78TzAf6+L+31H7jgSW1GDfq4D3o7YZsIE4iSDBGIdFbf8LcHsw\nPwdfRBbZdk7Fi1OFY88FLg3mzwY+rWTfvwE3BPOVJYK10f8WwE+j941x3CXAucF8VYngGeCBqG3t\n8PVC2VWdm2qe58uA+XH2+ywSb4X1iSSC1VXEMDryucDJwJdA0xj7nQR8DliwvAi4oK7/rhr7pKKh\n9PFF9IKZ9TWzvwe3+t8A9wNZlbz/y6j5XVReQRxv30Oj43D+L7c43kESjDGhzwLWVBIvwPPAmGD+\n0mA5Esd5ZjYvKLbYhv81Xtm5ijikshjM7AozKwiKN7YBfRM8Lvjvd+B4zrlvgK1Aj6h9Evo3q+I8\nH4a/4MdS2baqVPz/2N3MZprZuiCGP1WIocj5hgnlOOf+ib+7GG5mxwA9gb/XMKa0pUSQPio2nXwC\n/wv0SOdcO+Bu/C/0ZNqA/8UKgJkZ5S9cFdUmxg34C0hEVc1bZwLfN7Me+KKr54MYWwEvAr/GF9t0\nAP43wTi+jBeDmR0OPI4vHukcHPeTqONW1dR1Pb64KXK8tvgiqHUJxFVRZef5C+CIOO+Lt21nEFNm\n1LruFfap+P1+g2/tNiCI4YoKMfQys6Zx4ngW+DH+7mWmc+67OPtJHEoE6astsB3YGVS2XVcPn/k3\nYLCZnW9mzfDlzl2SFONM4BYz6xFUHN5R2c7OuS/xxRd/whcLrQw2tcSXW28C9pnZefiy7ERjuNPM\nOpjvZzEhalsb/MVwEz4nXou/I4jYCGRHV9pWMB242syONbOW+ET1vnMu7h1WJSo7z68CPc1sgpm1\nNLN2ZjY02PYHYLKZHWHeIDPrhE+AX+IbJTQ1s3FEJa1KYtgJbDezw/DFUxH/ArYAD5ivgG9lZidF\nbf8zvijpUnxSkGpSIkhftwGX4ytvn8BX6iaVc24jcDHwEP4P+whgIf6XYF3H+DgwC1gMzMf/qq/K\n8/gy/wPFQs65bcCtwMv4CtfR+ISWiHvwdyZFwBtEXaScc4XAY8BHwT5HAfOi3vs2sBLYaGbRRTyR\n97+JL8J5OXh/T2BsgnFVFPc8O+e2A2cBF+KT0wrg1GDzb4FX8Of5G3zFbUZQ5HctcCe+4cCRFb5b\nLPcAQ/EJ6VXgpagYSoHzgKPxdwdr8f8Oke1F+H/n75xzH1bzuwtlFSwi9S641V8PjHbOvR92PNJw\nmdmz+Aroe8OOpSFShzKpV2Y2At9CZze++eFe/K9ikRoJ6ltGAQPCjqWhUtGQ1LfhwGp82fi/AT9U\n5Z7UlJn9Gt+X4QHn3Nqw42moVDQkIpLmdEcgIpLmGlwdQVZWlsvJyQk7DBGRBmXBggWbnXMxm2s3\nuESQk5NDfn5+2GGIiDQoZha3d72KhkRE0pwSgYhImlMiEBFJc0oEIiJpTolARCTNJS0RmNlTZvaV\nmS2Js92CR9WtMrNCMxucrFhExJs2DXJyoEkT/zptWupsV2w1315ryXriDXAKMJjg6VQxtp+DH5HR\ngGHAvESOO2TIECeSyp57zrlevZwz86/PPZca2597zrnMTOegbMrMTI3tiq3m2xMF5Lt41+t4G+pi\nwj8rNV4ieAIYE7X8KXBIVcdUIpBU0BAvtr16lV8fmXr18u8Nc7tiq/n2RKVqIvgbMDxqeRbBw8Jj\n7DsOyAfye/bsWb1vLxJDbX51N9SLrVnsbWb+vWFuV2w1356oBp8IoifdEUgianohT2R7Q73YpnKS\nUmw1356oVE0EKhqSGqvNhb62f3gN9WKbysVWiq3m2xOVqong3AqVxR8lckwlAqnthb62v7ob6sU2\nsj0VK7LD/uyGHFuiQkkE+Idrb8A/gaoYuBq4Hrg+2G7AVOAz/PNGqywWckoE4mp/oa/tr+6GfLGV\n9BXaHUEyJiWC9BHvglbbC31d3IrrYisNjRKBpKRktcypiwu5LvTS2CgRSMqpTTm/frGLVF9liaDB\nPbM4NzfX6cE0DV9ODqyJ8ZiMXr2gqMh3pY/1X9MM9u/3XewnTYK1a6FnT5gyBcaOTXbUIg2XmS1w\nzuXG2tbgnlAmjcPatZWv79kzdqLo2dO/jh2rC79IXdHoo5I0lQ2UFbmgVxRZP2UKZGaW35aZ6deL\nSN1SIpAaq2q0xnHj/K965/zruHFl+1R1oR87FvLyfFGRmX/Ny9NdgEgyqI5AaiRyod+1q2xdZmbZ\nxbqqOoDIMVTOL1I/KqsjUCKQGqltZa+I1K/KEoGKhiSuyop+EqnsjSXeehEJjxKBxFRVGb8qe0Ua\nDyUCiWnSpPLl/+CXJ03y86rsFWk8VEcgMSVSxq/KXpGGQx3KpNqq6tAF6tQl0lioaCjNxasQVhm/\nSPrQHUEaq9gXIFIhDGW/9FX0I9L4qY4gjSXS6UtEGgf1I5CYquoLICLpQYmgkavNwG8ikh6UCBqx\n2g78JiLpQYmgEauqU5g6fYkIqLK4UdPAbyISocriNKU6ABFJhBJBI6Y6ABFJhBJBI6Y6ABFJhBJB\nA1dZ81DwF/2iIl8nUFSkJCAiB9MQEw1YIkNEiIhURXcEDVhVzUNFRBKhRNCAaYgIEakLSgQNmJqH\nikhdUCJowNQ8VETqghJBiqusVZCah4pIXVCroRSWSKsgPS5SRGpLdwQpTK2CRKQ+KBGkMLUKEpH6\noESQwtQqSETqgxJBClOrIBGpD0oEKUytgkSkPiQ1EZjZCDP71MxWmdnEGNt7mdksMys0s3+YWXYy\n42mINGiciCRb0hKBmTUFpgJnA/2AMWbWr8JuvwOedc4dC9wP/DpZ8aSqqkYPFRFJtmTeEQwFVjnn\nVjvn9gAzgFEV9ukHvBvMz46xvVGr6uHyIiL1IZmJoAfwRdRycbAuWgFwQTD/Q6CtmXWueCAzG2dm\n+WaWv2nTpqQEGwb1ExCRVBB2ZfHtwKlmthA4FVgH7Ku4k3MuzzmX65zL7dKlS33HmDTqJyAiqSCZ\nQ0ysAw6LWs4O1h3gnFtPcEdgZm2AC51z25IYU0rp2dMXB8VaLyJSX5J5RzAf6GNmvc2sBXAJ8Gr0\nDmaWZWaRGH4BPJXEeFKO+gmISCpIWiJwzpUCE4C3gOXATOfcUjO738xGBrudBnxqZiuAbkBaXQLV\nT0BEUoE558KOoVpyc3Ndfn5+2GGIiDQoZrbAOZcba5uGoW7k9u+HrVth3z5o2jT+ZBZ2pCISFiWC\nBqK0FHbuhJKS2NPmzfDll+WnDRtg40b/3qo0awYtWsSe2rWDIUNg2DA/9emjxCHSmCgRpLilS+Hm\nm2HWrKr3bdIEunaF7t39dMwx/rVbN2je3N8VxJv27oU9e2JPmzbBc8/B44/7z+nUCYYOLUsMffv6\nhNGsmf+c5s3L5puE3UBZRKqkRFAPpk3zncTWrvVNQ6dMqbpCeMcOuO8+eOQRaNsWJk6ELl2gTRu/\n3KZN2dS6NXTuDFlZvpgnGfbtg08+gblzy6b77vM9oitjBh06QI8esafsbJ+wmul/okhoVFmcZBUf\nNwm+iWi81kHOwfTpcPvtvnjnmmvggQf8RT7VfPMNzJ/vB8MrLfXT3r0Hv379Naxb56fiYl9ctX9/\n2XEOPxzuuAMuvxxatgzt64g0apVVFisRJFlOTuxOY716+QtotCVLYMIEeO89yM2FqVN9EUxjU1rq\nk9y6dbByJTz2GHz0kb9D+PnP4dprD+5fEYtzvm6keXOfQFq2VFGUSDxKBCFq0iR28YlZ2a/ir7+G\nyZPh0UehfXv49a/h6quTV8yTapyDd97xRWbvveeLwH72M/jpT31FdcTWrT5hzJvni6bmzfPnLlrz\n5pCR4aeWLaFjRzjySPje9/zUp49/7dpVFd6SXpQIQlTZHcG778LDD8Mf/wi7d/tfwg884Mv709UH\nH/iE8Oabvm7hqqv8xX7uXF9HAf4C3q+fr6g+5hifSL77Dr79tvzrd9/5iu6VK+Gzz3wxVUTbtj4h\nZGX5ZG0W+7VnTxgwwE/9+yd2pyKSipQIQhSrjqBlSxg4EPLz/a/+Sy+F227zFxvxFizwSfEvf/F3\nCJEWSiecAMcfX/5OIRGlpb6yfsUKnxhWrPDTtm0+kezff/Braakvvtu92x/DzN9dRBJDpFVWp05+\n6thRdRySupQIQjZtGtx5p78QtWzpf6m2bw/jx8ONN8Khh4YdYeoqKfGtosIqxtm3D1avhsWL/VRY\n6F9XrYpd5Ne6dVliaN++8jqLli3jt6aK3KlEc658Rfy+ffGTGPg7Kt3BSIR6FocsNxdatfLz3bvD\nrbf6Io+2bcONqyFo0ybcz2/a1Ncr9OkDF1xQtn7XLn9HsXmzL7rassW/Rk/bt5dvHVXRli0+sVRs\nRQW+rqN164Mv/NWVkeGTSqR5cWS+QwefNCL9SCLHj0z798dOMJFXs4P7jET3I6msF3uzZj5BRTeB\nrtgcOvo4lf0IiHyH6H4v4JNsRobv36K6oKopESTZV1/BOef4fgEzZsCFF6rNfGOQmQmDBtXNsaJb\nUUVPu3bFvtA2a+anpk3L12dEzzvni702b/YJJ/K6cKGf377d7xs5TsUpcrx49SfOlW8iXHG+sgRY\nXZHvG/nuZuU7QFZVqBFJCpGWZWaVd65s2dL/cIs1ZWRUfpfXrJkvtmzfPvbkXPl/j82by+a3bPGf\nccghvpQg1muHDslJbLokJdHu3TBqFKxfD//4hy/fFqmoWTPfsS47O+xI6k7kziHWhba01P9tlJT4\nH0gVh0vZuTN2f5RIX5X9++MPh9KiRVnjgUjDgehGBN9+6+OLd7fSpInfd/fug6dt28reH8/evb5/\nzfbt/rtUpUOHsru1rl395xQW+sYSO3YcvP+jj/ri5LqmRJAk+/fDZZf5Jo4vvqgkIOnFrOzimq72\n7StLCpEJyornOnXydznxlJT48cI2bPA/Jtevh1NOSU6sSgRJMnEivPQS/P735cuWRSQ9NG3qW5J1\n7Fiz97dpU1Y/lWzqh5kETzwBv/2t7xB1661hRyMiUjklgjr2xhtwww2+gviRR9RiQURSnxJBHSoo\ngIsu8p2NXnhBrYNEpGFQIqgjxcVw7rm+idjf/hZ++3cRkUTpN2sdKC2FkSN9C4EPPvA9Q0VEGgol\ngjowe7bvqPPss3DssWFHIyJSPSoaqgOTJ/tK4Z/8xI82Om1a2BGJiCROdwS19PTTMGdO2fKaNX60\nUaj6cZQiIqlAdwS1dMcdB6/btcs/o1hEpCFQIqilTZtir1+7tn7jEBGpKSWCWtixI36HsZ496zcW\nEZGaUiKohb/+1Y90WPGpVJmZ/nGLIiINQZWJwMxuNLMaDpvUuD3/vP/l/+ST/hnEZv41L08VxSLS\ncCTSaqgbMN/MPgaeAt5yDe35lkmweTO8/Tb87Gd+uOnLLgs7IhGRmqnyjsA5dxfQB/gjcAWw0swe\nMLMjkhxbSnvxRd+j+NJLw45ERKR2EqojCO4AvgymUqAj8KKZPZjE2FLa9Olw9NHqSSwiDV8idQQ3\nm9kC4EHgn8AA59x4YAhwYZLjS0lffOE7kY0Zo2GmRaThS6SOoBNwgXNuTfRK59x+MzsvOWGlthde\n8K9jxoQbh4hIXUikaOgN4OvIgpm1M7MTAJxzy5MVWCqbPh2OPx6OPDLsSEREai+RRPA4UBK1XBKs\nS0uffgoff6y7ARFpPBJJBBbdXNQ5t580Hqxu+nRfL3DxxWFHIiJSNxJJBKvN7CYzax5MNwOrkx1Y\nKnLOJ4LTToNDDw07GhGRupFIIrge+D/AOqAYOAEYl8jBzWyEmX1qZqvMbGKM7T3NbLaZLTSzQjM7\npzrB17eFC2HFChULiUjjUmURj3PuK+CS6h7YzJoCU4Gz8Alkvpm96pxbFrXbXcBM59zjZtYPeB3I\nqe5n1Zfp06F5c7gwLRvNikhjVWUiMLMM4GqgP5ARWe+cu6qKtw4FVjnnVgfHmQGMAqITgQPaBfPt\ngfUJR17P9u+HGTNgxAjo1CnsaERE6k4iRUN/BroD/wa8B2QDOxJ4Xw/gi6jl4mBdtHuBH5tZMf5u\n4MZYBzKzcWaWb2b5m+I9ACDJPvgAiotVLCQijU8iieBI59wvgZ3OuWeAc/H1BHVhDPAn51w2cA7w\nZzM7KCbnXJ5zLtc5l9ulS5c6+ujqmT7dDy89cmQoHy8ikjSJJIK9wes2MzsGX4TTNYH3rQMOi1rO\nDtZFuxqYCeCc+xe+6CkrgWPXu7//Hc45B1q3DjsSEZG6lUgiyAueR3AX8Cq+jP83CbxvPtDHzHqb\nWQt8hfOrFfZZC5wJYGZH4xNBOGU/lfj6az++0NChYUciIlL3Kq0sDoppvnHObQXmAIcnemDnXKmZ\nTQDeApoCTznnlprZ/UC+c+5V4DbgSTO7FV9xfEUqPuugsNC/aqRREWmMKk0EwcBy/05QfFNdzrnX\n8ZXA0evujppfBpxUk2PXp0giGDgw3DhERJIhkaKhd8zsdjM7zMw6RaakR5ZCCguhSxfo1i3sSERE\n6l4iYwZFRtW5IWqdoxrFRA1dQYEvFtKzB0SkMUqkZ3Hv+ggkVe3bB0uWwE9/GnYkIiLJkUjP4p/E\nWu+ce7buw0k9K1fCt9+qolhEGq9EioaOj5rPwDf3/BhIi0SgimIRaewSKRoqN+yDmXUAZiQtohRT\nWAhNm/oH1YuINEaJtBqqaCeQNvUGBQXQty+0bBl2JCIiyZFIHcFr+FZC4BNHP2rYr6AhKiyE4cPD\njkJEJHkSqSP4XdR8KbDGOVecpHhSytatsHatKopFpHFLJBGsBTY4574FMLNWZpbjnCtKamQpYPFi\n/6qKYhFpzBKpI/gfYH/U8r5gXaOnMYZEJB0kkgiaOef2RBaC+RbJCyl1FBRA587w7ruQkwNNmvjX\nadPCjkxEpO4kkgg2mdmBx7GY2Shgc/JCSh2FhX58oeuugzVrwDn/Om6ckoGINB5W1ajPZnYEMA04\nNFhVDPzEObcqybHFlJub6/Lz85P+Ofv2Qdu20KwZ7IjxYM5evaCoKOlhiIjUCTNb4JzLjbUtkQ5l\nnwHDzKxNsFxSx/GlpM8+g927429fu7b+YhERSaYqi4bM7AEz6+CcK3HOlZhZRzObXB/BhSlSUdy9\ne+ztPXvWXywiIsmUSB3B2c65bZGF4Gll5yQvpNRQUOCHlnjgAf/Q+miZmTBlSjhxiYjUtUQSQVMz\nOzDAgpm1Ahr9gAuFhXDUUXDllZCX5+sEzPxrXh6MHRt2hCIidSORDmXTgFlm9jRgwBXAM8kMKhUU\nFMCJJ/r5sWN14ReRxqvKOwLn3G+AycDRwFH4h9H3SnJcodq+3TcTVY9iEUkHiY4+uhE/8NyPgDOA\n5UmLKAVEhpZQj2IRSQdxi4bM7HvAmGDaDLyA73dwej3FFpqCAv+qOwIRSQeV1RF8ArwPnBfpPGZm\nt9ZLVCErLIROneDQQ6veV0SkoausaOgCYAMw28yeNLMz8ZXFjV5BgS8WsrT4tiKS7uImAufcK865\nS4C+wGzgFqCrmT1uZj+orwDr2/79vo5AxUIiki4SaTW00zn3vHPufCAbWAjckfTIQrJ6NezapYpi\nEUkf1XpmsXNuq3Muzzl3ZrICCpsqikUk3dTk4fWNWmGhf+5Av35hRyIiUj+UCCooKIDvfQ9atQo7\nEhGR+qFEUEFhoYqFRCS9KBFE+eYb+PxzVRSLSHpRIogSGVpCdwQikk6UCKJEHkajOwIRSSdKBFEK\nCqBDB8jODjsSEZH6o0QQJVJRrKElRCSdKBEEIkNLqFhIRNKNEkHg88+hpEQVxSKSfpKaCMxshJl9\namarzGxijO3/aWaLgmmFmW1LZjyVUUWxiKSrRJ5ZXCNm1hSYCpwFFAPzzexV59yyyD7OuVuj9r8R\nOC5Z8VSloMAPLdG/f1gRiIiEI5l3BEOBVc651c65PcAMYFQl+48BpicxnkotXAh9+kBmZlgRiIiE\nI5mJoAfwRdRycbDuIGbWC+gNvBtn+zgzyzez/E2bNtV5oM7BvHlwwgl1fmgRkZSXKpXFlwAvOuf2\nxdoYDH2d65zL7dKlS51/+Jo1sHEjDBtW54cWEUl5yUwE64DDopazg3WxXEKIxUJz5/pXJQIRSUfJ\nTATzgT5m1tvMWuAv9q9W3MnM+gIdgX8lMZZKzZ3rh50eMCCsCEREwpO0ROCcKwUmAG8By4GZzrml\nZna/mY2M2vUSYIZzziUrlqrMnQvHHw/NktaGSkQkdSX10uecex14vcK6uyss35vMGKry3Xe+xdAt\nt4QZhYhIeFKlsjg0CxfCnj2qHxCR9JX2iSBSUaymoyKSrpQI5kLPnnDooWFHIiISDiWCuSoWEpH0\nltaJ4MsvfWeyZs0gJ8ePNZSTA9OmhR2ZiEj9SesGk/Pm+deXXvKth8AnhnHj/PzYseHEJSJSn9L6\njiBSURxJAhG7dsGkSfUfj4hIGJQI4li7tv7iEBEJU9omgtJSmD8f2raNvb1nz/qNR0QkLGmbCJYu\nhZ074bLLDn4GQWYmTJkSTlwiIvUtbRNBpFjottsgLw969QIz/5qXp4piEUkfadtqaO5c6NIFeveG\nww/XhV9E0lda3xEMG+bvAkRE0llaJoKtW+GTT9SjWEQE0jQRfPSRf1UiEBFJ00Qwd64vEjr++LAj\nEREJX9omgmOOid+HQEQknaRdIti/348xpOcPiIh4aZcIVq70lcWqHxAR8dIuEUQ6kikRiIh4aZkI\n2rWDo48OOxIRkdSQlolg6FD/EBoREUmzRLBzJxQWqlhIRCRaWiWC/HzfakiJQESkTFolgkhFsZqO\nioiUSbtEcOSRkJUVdiQiIqkjbYahds4ngu9/P+xIRBqmvXv3UlxczLfffht2KFKJjIwMsrOzad68\necLvSZtEsHYtfPml6gdEaqq4uJi2bduSk5ODafz2lOScY8uWLRQXF9O7d++E35c2RUPqSCZSO99+\n+y2dO3dWEkhhZkbnzp2rfdeWNolgyxbo2hWOPTbsSEQaLiWB1FeTf6O0SQQ//akvGqpGsZmISFpI\nm0QAeiylSH2aNg1ycnwv/pwcv1wbW7ZsYdCgQQwaNIju3bvTo0ePA8t79uxJ6BhXXnkln376aaX7\nTJ06lWm1DbaBSZvKYhGpP9OmwbhxsGuXX16zxi8DjB1bs2N27tyZRYsWAXDvvffSpk0bbr/99nL7\nOOdwztEkzhgyTz/9dJWfc8MNN9QswAYsre4IRKR+TJpUlgQidu3y6+vaqlWr6NevH2PHjqV///5s\n2LCBcePGkZubS//+/bn//vsP7Dt8+HAWLVpEaWkpHTp0YOLEiQwcOJATTzyRr776CoC77rqLhx9+\n+MD+EydOZOjQoRx11FF8+OGHAOzcuZMLL7yQfv36MXr0aHJzcw8kqWj33HMPxx9/PMcccwzXX389\nzjkAVqxYwRlnnMHAgQMZPHgwRUVFADzwwAMMGDCAgQMHMikZJysOJQIRqXNr11ZvfW198skn3Hrr\nrSxbtowePXrwH//xH+Tn51NQUMDbb7/NsmXLDnrP9u3bOfXUUykoKODEE0/kqaeeinls5xwfffQR\nv/3tbw8klccee4zu3buzbNkyfvnLX7Jw4cKY77355puZP38+ixcvZvv27bz55psAjBkzhltvvZWC\nggI+/PBDunbtymuvvcYbb7zBRx99REFBAbfddlsdnZ2qKRGISJ3r2bN662vriCOOIDc398Dy9OnT\nGTx4MIMHD2b58uUxE0GrVq04++yzARgyZMiBX+UVXXDBBQft88EHH3DJJZcAMHDgQPr37x/zvbNm\nzWLo0KEMHDiQ9957j6VLl7J161Y2b97M+eefD/gOYJmZmbzzzjtcddVVtGrVCoBOnTpV/0TUUFIT\ngZmNMLNPzWyVmU2Ms89FZuTuj7UAAA4WSURBVLbMzJaa2fPJjEdE6seUKZCZWX5dZqZfnwytW7c+\nML9y5UoeeeQR3n33XQoLCxkxYkTMdvUtWrQ4MN+0aVNKS0tjHrtly5ZV7hPLrl27mDBhAi+//DKF\nhYVcddVVKdsrO2mJwMyaAlOBs4F+wBgz61dhnz7AL4CTnHP9gVuSFY+I1J+xYyEvD3r18q31evXy\nyzWtKK6Ob775hrZt29KuXTs2bNjAW2+9VeefcdJJJzFz5kwAFi9eHPOOY/fu3TRp0oSsrCx27NjB\nSy+9BEDHjh3p0qULr732GuA76u3atYuzzjqLp556it27dwPw9ddf13nc8SSz1dBQYJVzbjWAmc0A\nRgHRZ+xaYKpzbiuAc+6rJMYjIvVo7Nj6ufBXNHjwYPr160ffvn3p1asXJ510Up1/xo033shPfvIT\n+vXrd2Bq3759uX06d+7M5ZdfTr9+/TjkkEM4IWrY42nTpnHdddcxadIkWrRowUsvvcR5551HQUEB\nubm5NG/enPPPP59f/epXdR57LBapxa7zA5uNBkY4564Jli8DTnDOTYja5xVgBXAS0BS41zn3ZmXH\nzc3Ndfn5+UmJWUTiW758OUfrGa8AlJaWUlpaSkZGBitXruQHP/gBK1eupFmz1GiRH+vfyswWOOdy\nY+0fdtTNgD7AaUA2MMfMBjjntkXvZGbjgHEAPZNV2yQikqCSkhLOPPNMSktLcc7xxBNPpEwSqIlk\nRr4OOCxqOTtYF60YmOec2wt8bmYr8IlhfvROzrk8IA/8HUHSIhYRSUCHDh1YsGBB2GHUmWS2GpoP\n9DGz3mbWArgEeLXCPq/g7wYwsyzge8DqJMYkIiIVJC0ROOdKgQnAW8ByYKZzbqmZ3W9mI4Pd3gK2\nmNkyYDbwc+fclmTFJCIiB0tqoZZz7nXg9Qrr7o6ad8DPgklEREKgnsUiImlOiUBEGoTTTz/9oM5h\nDz/8MOPHj6/0fW3atAFg/fr1jB49OuY+p512GlU1S3/44YfZFTWS3jnnnMO2bdsqeUfDoUQgIg3C\nmDFjmDFjRrl1M2bMYMyYMQm9/9BDD+XFF1+s8edXTASvv/46HTp0qPHxUknDbfgqIqG55RaIMepy\nrQwaBMHozzGNHj2au+66iz179tCiRQuKiopYv349J598MiUlJYwaNYqtW7eyd+9eJk+ezKhRo8q9\nv6ioiPPOO48lS5awe/durrzySgoKCujbt++BYR0Axo8fz/z589m9ezejR4/mvvvu49FHH2X9+vWc\nfvrpZGVlMXv2bHJycsjPzycrK4uHHnrowOil11xzDbfccgtFRUWcffbZDB8+nA8//JAePXrw17/+\n9cCgchGvvfYakydPZs+ePXTu3Jlp06bRrVs3SkpKuPHGG8nPz8fMuOeee7jwwgt58803ufPOO9m3\nbx9ZWVnMmjWr1udeiUBEGoROnToxdOhQ3njjDUaNGsWMGTO46KKLMDMyMjJ4+eWXadeuHZs3b2bY\nsGGMHDky7vN7H3/8cTIzM1m+fDmFhYUMHjz4wLYpU6bQqVMn9u3bx5lnnklhYSE33XQTDz30ELNn\nzyYrK6vcsRYsWMDTTz/NvHnzcM5xwgkncOqpp9KxY0dWrlzJ9OnTefLJJ7nooot46aWX+PGPf1zu\n/cOHD2fu3LmYGX/4wx948MEH+f3vf8+vfvUr2rdvz+LFiwHYunUrmzZt4tprr2XOnDn07t27zsYj\nUiIQkWqr7Jd7MkWKhyKJ4I9//CPgnxlw5513MmfOHJo0acK6devYuHEj3bt3j3mcOXPmcNNNNwFw\n7LHHcuyxxx7YNnPmTPLy8igtLWXDhg0sW7as3PaKPvjgA374wx8eGAH1ggsu4P3332fkyJH07t2b\nQYMGAfGHui4uLubiiy9mw4YN7Nmzh969ewPwzjvvlCsK69ixI6+99hqnnHLKgX3qaqjqtKgjqOtn\np4pIOEaNGsWsWbP4+OOP2bVrF0OGDAH8IG6bNm1iwYIFLFq0iG7dutVoyOfPP/+c3/3ud8yaNYvC\nwkLOPffcWg0dHRnCGuIPY33jjTcyYcIEFi9ezBNPPBHKUNWNPhFEnp26Zg04V/bsVCUDkYanTZs2\nnH766Vx11VXlKom3b99O165dad68ObNnz2bNmjWVHueUU07h+ef940+WLFlCYWEh4Iewbt26Ne3b\nt2fjxo288cYbB97Ttm1bduzYcdCxTj75ZF555RV27drFzp07efnllzn55JMT/k7bt2+nR48eADzz\nzDMH1p911llMnTr1wPLWrVsZNmwYc+bM4fPPPwfqbqjqRp8I6vPZqSKSfGPGjKGgoKBcIhg7diz5\n+fkMGDCAZ599lr59+1Z6jPHjx1NSUsLRRx/N3XfffeDOYuDAgRx33HH07duXSy+9tNwQ1uPGjWPE\niBGcfvrp5Y41ePBgrrjiCoYOHcoJJ5zANddcw3HHHZfw97n33nv50Y9+xJAhQ8rVP9x1111s3bqV\nY445hoEDBzJ79my6dOlCXl4eF1xwAQMHDuTiiy9O+HMqk7RhqJOlusNQN2ni7wQqMoP9++swMJFG\nTsNQNxzVHYa60d8R1PezU0VEGppGnwjq+9mpIiINTaNPBGE+O1WksWloRcnpqCb/RmnRjyCsZ6eK\nNCYZGRls2bKFzp07x+2oJeFyzrFlyxYyMjKq9b60SAQiUnvZ2dkUFxezadOmsEORSmRkZJCdnV2t\n9ygRiEhCmjdvfqBHqzQujb6OQEREKqdEICKS5pQIRETSXIPrWWxmm4B4A4lkAZvrMZzqSuX4FFvN\nKLaaUWw1U5vYejnnusTa0OASQWXMLD9eF+pUkMrxKbaaUWw1o9hqJlmxqWhIRCTNKRGIiKS5xpYI\n8sIOoAqpHJ9iqxnFVjOKrWaSElujqiMQEZHqa2x3BCIiUk1KBCIiaa7RJAIzG2Fmn5rZKjObGHY8\n0cysyMwWm9kiM0v88WrJieUpM/vKzJZEretkZm+b2crgtWMKxXavma0Lzt0iMzsnpNgOM7PZZrbM\nzJaa2c3B+tDPXSWxhX7uzCzDzD4ys4IgtvuC9b3NbF7w9/qCmbVIodj+ZGafR523QfUdW1SMTc1s\noZn9LVhOznlzzjX4CWgKfAYcDrQACoB+YccVFV8RkBV2HEEspwCDgSVR6x4EJgbzE4HfpFBs9wK3\np8B5OwQYHMy3BVYA/VLh3FUSW+jnDjCgTTDfHJgHDANmApcE6/8bGJ9Csf0JGB32/7kgrp8BzwN/\nC5aTct4ayx3BUGCVc261c24PMAMYFXJMKck5Nwf4usLqUcAzwfwzwP+t16ACcWJLCc65Dc65j4P5\nHcByoAcpcO4qiS10zisJFpsHkwPOAF4M1od13uLFlhLMLBs4F/hDsGwk6bw1lkTQA/giarmYFPlD\nCDjgf81sgZmNCzuYGLo55zYE818C3cIMJoYJZlYYFB2FUmwVzcxygOPwvyBT6txViA1S4NwFxRuL\ngK+At/F379ucc6XBLqH9vVaMzTkXOW9TgvP2n2bWMozYgIeBfwf2B8udSdJ5ayyJINUNd84NBs4G\nbjCzU8IOKB7n7zlT5lcR8DhwBDAI2AD8PsxgzKwN8BJwi3Pum+htYZ+7GLGlxLlzzu1zzg0CsvF3\n733DiCOWirGZ2THAL/AxHg90Au6o77jM7DzgK+fcgvr4vMaSCNYBh0UtZwfrUoJzbl3w+hXwMv6P\nIZVsNLNDAILXr0KO5wDn3Mbgj3U/8CQhnjsza46/0E5zzv0lWJ0S5y5WbKl07oJ4tgGzgROBDmYW\neTBW6H+vUbGNCIranHPuO+BpwjlvJwEjzawIX9R9BvAISTpvjSURzAf6BDXqLYBLgFdDjgkAM2tt\nZm0j88APgCWVv6vevQpcHsxfDvw1xFjKiVxkAz8kpHMXlM/+EVjunHsoalPo5y5ebKlw7sysi5l1\nCOZbAWfh6zBmA6OD3cI6b7Fi+yQqsRu+DL7ez5tz7hfOuWznXA7+evauc24syTpvYdeK19UEnINv\nLfEZMCnseKLiOhzfiqkAWBp2bMB0fDHBXnwZ49X4ssdZwErgHaBTCsX2Z2AxUIi/6B4SUmzD8cU+\nhcCiYDonFc5dJbGFfu6AY4GFQQxLgLuD9YcDHwGrgP8BWqZQbO8G520J8BxBy6KwJuA0yloNJeW8\naYgJEZE011iKhkREpIaUCERE0pwSgYhImlMiEBFJc0oEIiJpTolAJGBm+6JGnFxkdTiKrZnlRI+q\nKpJKmlW9i0ja2O38cAMiaUV3BCJVMP88iQfNP1PiIzM7MlifY2bvBoOTzTKznsH6bmb2cjDOfYGZ\n/Z/gUE3N7Mlg7Pv/DXqzYmY3Bc8SKDSzGSF9TUljSgQiZVpVKBq6OGrbdufcAOC/8KNCAjwGPOOc\nOxaYBjwarH8UeM85NxD/fIWlwfo+wFTnXH9gG3BhsH4icFxwnOuT9eVE4lHPYpGAmZU459rEWF8E\nnOGcWx0M7valc66zmW3GD9uwN1i/wTmXZWabgGznBy2LHCMHP8xxn2D5DqC5c26ymb0JlACvAK+4\nsjHyReqF7ghEEuPizFfHd1Hz+yirozsXmIq/e5gfNbqkSL1QIhBJzMVRr/8K5j/EjwwJMBZ4P5if\nBYyHAw8+aR/voGbWBDjMOTcbP+59e+CguxKRZNIvD5EyrYKnVUW86ZyLNCHtaGaF+F/1Y4J1NwJP\nm9nPgU3AlcH6m4E8M7sa/8t/PH5U1ViaAs8FycKAR50fG1+k3qiOQKQKQR1BrnNuc9ixiCSDioZE\nRNKc7ghERNKc7ghERNKcEoGISJpTIhARSXNKBCIiaU6JQEQkzf1/CR/yr3jtYosAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx--Ytk3ZbLo"
      },
      "source": [
        "The accuracy of model3 with an additional layer is 85%. Adding more layers can help you to extract more features. But we can do that upto a certain extent. After some point, instead of extracting features, we tend to overfit the data. Overfitting can lead to errors in some or the other form like false positives. It is not easy to choose the number of units in a hidden layer or the number of hidden layers in a neural network. For many applications, one hidden layer is enough. As a general rule, the number of units in that hidden layer is between the number of inputs and the number of outputs.\n",
        " The best way to decide on the number of units and hidden layers is to try various parameters. Train several neural networks with different numbers of hidden layers and neurons, and monitor the performance of them. You will have to experiment using a series of different architectures. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gn2GSV4ioyO2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XYC6DykEox2w",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GsCJ01StlgCx"
      },
      "source": [
        "This tutorial is substantially based on this document:\n",
        "https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
        "\n",
        "To read more about Sequential APIs you can go to: https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "The one-hot word vector layer is taken from:\n",
        "https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jL0UovfaE9GE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "LAB6_TASK_B.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYHtu_zB_KAL",
        "colab_type": "code",
        "outputId": "92d87b17-49df-4044-92c8-94935c3fcc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhJduOzM-5f9",
        "colab_type": "code",
        "outputId": "4d5a2c39-25f1-438a-a86f-48aed09dc009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import string #preprocess\n",
        "import csv   #reading files\n",
        "import emoji #to convert emojis to words using demojize function\n",
        "import keras\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "#preprocess function\n",
        "import string\n",
        "def preprocess(list_of_tweets):\n",
        "  normalised_corpus =[]\n",
        "  for tweet in list_of_tweets:\n",
        "    words =[]\n",
        "    for word in tweet:\n",
        "      word = word.lower()\n",
        "      word = word.strip(string.punctuation)\n",
        "      word = word.strip(string.digits)\n",
        "      if word not in string.punctuation and word != \"url\": \n",
        "        words.append(word)\n",
        "    normalised_corpus.append(words)\n",
        "  return normalised_corpus  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6fvoMIC-5gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6_5OGu5-5gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting train data\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "raw_data =[] #stores all contents of tsv file as list of list [[id,tweet,A,B,C],[...]]\n",
        "with open(\"olid-training-v1.0.tsv\",encoding='UTF-8') as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        raw_data.append(row)\n",
        "\n",
        "raw_data.remove(raw_data[0]) #remove first list item since it contains headers\n",
        "\n",
        "#converts emoji to words and converts each tweet from string to list of words\n",
        "raw_tweets = []\n",
        "for tweet in raw_data:\n",
        "  tweet[1] = emoji.demojize(tweet[1])\n",
        "  temp = tweet[1].split()\n",
        "  raw_tweets.append(temp)\n",
        "\n",
        " #convert to lower, remove punctuation and digits and words like 'user' and 'url' \n",
        "clean_tweets = preprocess(raw_tweets)\n",
        "\n",
        "num_words=20000  #optional UNIQUE WORDS around 22156\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_tweets)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = tokenizer.index_word\n",
        "sents_as_ids = tokenizer.texts_to_sequences(clean_tweets)\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAXIMUM_LENGTH = 30 #checking\n",
        "processed_train_data = pad_sequences(sents_as_ids,MAXIMUM_LENGTH,truncating='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWRtmsSt-5gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GETTING ALL THE LABELS\n",
        "train_labels_A = [tweet[2] for tweet in raw_data ]\n",
        "train_labels_B = [tweet[3] for tweet in raw_data ]\n",
        "train_labels_C = [tweet[4] for tweet in raw_data ]\n",
        "combined_train_labels = []  #combining all labels \n",
        "\n",
        "#print(train_labels_A)\n",
        "preprocessed_label_A = []\n",
        "for labelA in train_labels_A:\n",
        "  if labelA ==\"OFF\":\n",
        "    preprocessed_label_A.append(1)\n",
        "  else:\n",
        "    preprocessed_label_A.append(0)\n",
        "\n",
        "for i in range(len(train_labels_A)):\n",
        "  if train_labels_A[i] ==\"OFF\":\n",
        "    if train_labels_C[i] !=\"NULL\":\n",
        "      temp = train_labels_A[i] + train_labels_B[i] + train_labels_C[i] \n",
        "    else:\n",
        "      temp = train_labels_A[i] + train_labels_B[i]\n",
        "  else:\n",
        "    temp = train_labels_A[i]\n",
        "  combined_train_labels.append(temp)\n",
        "\n",
        "#sanity check\n",
        "#print(\"first 10 combined train labels: \",combined_train_labels[:10])\n",
        "\n",
        "unique_labels = {}\n",
        "count =0\n",
        "for label in combined_train_labels:\n",
        "  if label not in unique_labels.keys():\n",
        "    unique_labels[label] = count\n",
        "    count+=1\n",
        "\n",
        "#print(unique_labels)\n",
        "\n",
        "labels_to_ids = []\n",
        "\n",
        "for label in combined_train_labels:\n",
        "  labels_to_ids.append(unique_labels[label])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxzvsl_K-5gN",
        "colab_type": "code",
        "outputId": "89de4380-ae56-4820-9180-4293c7b4cc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "#CONVERTS ALL LABELS TO ONE HOT ENCODING [1,0,0,0,0] for all 5 classes\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels_to_ids)\n",
        "encoded_Y = encoder.transform(labels_to_ids)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "processed_train_labels = np_utils.to_categorical(encoded_Y)\n",
        "print(\"one hot encoded train labels \\n \",processed_train_labels[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one hot encoded train labels \n",
            "  [[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpM6Ag2qZyN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove\n",
        "\n",
        "    \n",
        "from keras.initializers import Constant\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWKO6vcBZ_9m",
        "colab_type": "code",
        "outputId": "b52743cc-f2d6-4754-ded9-c585c86c1b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6eVqmyjaLUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "wordToIndex, indexToWord, wordToGlove = readGloveFile('drive/My Drive/Colab Notebooks/glove.6B.300d.txt')  #glove.twitter.27B.200d.txt  glove.6B.300d.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOnEjC1P-5gP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(word2idx)+1  # 0 saved for padding so we add 1\n",
        "EMBD_SIZE =100\n",
        "output = len(unique_labels.keys())\n",
        "model2 = keras.Sequential()\n",
        "#model2.add(createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,False)) #setting isTrainable to true \n",
        "model2.add(Embedding(VOCAB_SIZE,EMBD_SIZE))\n",
        "model2.add(keras.layers.Conv1D(filters=128,kernel_size=5,padding='SAME',activation='tanh'))\n",
        "#model2.add(keras.layers.Conv1D(filters=128,kernel_size=5,padding='VALID',activation='sigmoid'))\n",
        "#model2.add(keras.layers.GlobalAveragePooling1D())\n",
        "model2.add(keras.layers.GlobalMaxPooling1D())\n",
        "model2.add(keras.layers.Dense(16, activation='sigmoid'))\n",
        "model2.add(keras.layers.Dense(5, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUQNJNTCXi5t",
        "colab_type": "code",
        "outputId": "f4a8f6dc-7265-426f-a121-72b4ffa519d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 100)         2215800   \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 85        \n",
            "=================================================================\n",
            "Total params: 2,282,077\n",
            "Trainable params: 2,282,077\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYBUQvT-5gS",
        "colab_type": "code",
        "outputId": "cb8259ed-9eab-4eeb-fe6e-f1b90d525e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "partial_train_dataABC = processed_train_data[:10240]\n",
        "partial_val_dataABC = processed_train_data[10240:]\n",
        "\n",
        "partial_train_labels_ABC = processed_train_labels[:10240]\n",
        "partial_val_labels_ABC = processed_train_labels[10240:]\n",
        "\n",
        "history = model2.fit(partial_train_dataABC,partial_train_labels_ABC,batch_size=100,\n",
        "            epochs=6,validation_data=(partial_val_dataABC,partial_val_labels_ABC),verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10240 samples, validate on 3000 samples\n",
            "Epoch 1/6\n",
            "10240/10240 [==============================] - 7s 655us/step - loss: 1.2318 - acc: 0.5484 - val_loss: 1.0194 - val_acc: 0.6683\n",
            "Epoch 2/6\n",
            "10240/10240 [==============================] - 6s 609us/step - loss: 1.0031 - acc: 0.6675 - val_loss: 0.9872 - val_acc: 0.6683\n",
            "Epoch 3/6\n",
            "10240/10240 [==============================] - 6s 604us/step - loss: 0.8709 - acc: 0.7142 - val_loss: 0.8884 - val_acc: 0.7103\n",
            "Epoch 4/6\n",
            "10240/10240 [==============================] - 6s 614us/step - loss: 0.6471 - acc: 0.8100 - val_loss: 0.9205 - val_acc: 0.6917\n",
            "Epoch 5/6\n",
            "10240/10240 [==============================] - 6s 614us/step - loss: 0.4748 - acc: 0.8660 - val_loss: 0.9758 - val_acc: 0.6807\n",
            "Epoch 6/6\n",
            "10240/10240 [==============================] - 6s 611us/step - loss: 0.3605 - acc: 0.8979 - val_loss: 1.0337 - val_acc: 0.6757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7BOyiR8_nr-",
        "colab_type": "code",
        "outputId": "bcd5ca08-e1c0-4483-ab21-24b696a2760c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# PLOT of train and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fgl3OnV-5gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GET RAW TEST DATA AND CONVERT TO DICTIONERY WITH KEY (ID AS INT) AND VALUE =TWEET \n",
        "raw_test_data =[]\n",
        "\n",
        "with open(\"testset-levela.tsv\",encoding='UTF-8') as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        raw_test_data.append(row)\n",
        "        \n",
        "raw_test_data.remove(raw_test_data[0]) #remove first element since it contains headers       \n",
        "\n",
        "raw_test_data_dict =dict(raw_test_data)\n",
        "raw_test_data_dict = {int(k) : v for k,v in raw_test_data_dict.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOKntR93-5gX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#processing the test labels\n",
        "#converting labels to key value pairs\n",
        "def get_test_labels(path):\n",
        "  raw_test_labels =[]\n",
        "  with open(path) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        raw_test_labels.append(row)\n",
        "  temp_test_labels = [labels[0].split(',') for labels in raw_test_labels]\n",
        "  return temp_test_labels\n",
        "\n",
        "#get labels with IDs\n",
        "test_labels_A = get_test_labels('labels-levela.csv')\n",
        "test_labels_B = get_test_labels('labels-levelb.csv')\n",
        "test_labels_C = get_test_labels('labels-levelc.csv')\n",
        "\n",
        "dict1 =dict(test_labels_A)\n",
        "dict1 = {int(k) : v for k,v in dict1.items()}\n",
        "dict2 =dict(test_labels_B)\n",
        "dict2 = {int(k) : v for k,v in dict2.items()}\n",
        "dict3 =dict(test_labels_C)\n",
        "dict3 = {int(k) : v for k,v in dict3.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHWzuIGl-5gZ",
        "colab_type": "code",
        "outputId": "0314048d-f8d2-4e3a-faf6-4e47e4c46e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#this list will store tweet and corrosponding concatenated labels so its a list of list containing tweet and label\n",
        "combined_test_data = [] \n",
        "for key in raw_test_data_dict.keys():\n",
        "    raw_test_data_dict[key] = emoji.demojize(raw_test_data_dict[key]) #demojize\n",
        "    l1 = raw_test_data_dict[key].split()\n",
        "    if key in dict3.keys():\n",
        "        label_temp = dict1[key]+dict2[key]+dict3[key]\n",
        "        tweet = [l1,label_temp]\n",
        "    elif key in dict2.keys():     \n",
        "        label_temp = dict1[key]+dict2[key]\n",
        "        tweet = [l1,label_temp]\n",
        "    elif key in dict1.keys():          \n",
        "        tweet = [l1,dict1[key]]\n",
        "    combined_test_data.append(tweet)\n",
        "\n",
        "raw_test_tweets =[]\n",
        "raw_test_labels =[]\n",
        "for list1 in combined_test_data:\n",
        "    raw_test_tweets.append(list1[0])\n",
        "    raw_test_labels.append(list1[1])\n",
        "\n",
        "clean_test_tweets = preprocess(raw_test_tweets)\n",
        "test_sents_as_ids = tokenizer.texts_to_sequences(clean_test_tweets)\n",
        "\n",
        "\n",
        "\n",
        "MAXIMUM_LENGTH = 30 #checking\n",
        "processed_test_data = pad_sequences(test_sents_as_ids,MAXIMUM_LENGTH,truncating='post')\n",
        "\n",
        "\n",
        "test_labels_to_ids = []\n",
        "\n",
        "for label in raw_test_labels:\n",
        "  test_labels_to_ids.append(unique_labels[label])\n",
        "\n",
        "\n",
        "#CONVERT TEST LABELS TO ONE HOT LABEL ENCODING\n",
        "encoded_testY = encoder.transform(test_labels_to_ids)\n",
        "processed_test_labels = np_utils.to_categorical(encoded_testY)\n",
        "\n",
        "#sanity check\n",
        "print(test_labels_to_ids[:10])\n",
        "print(\"one hot encoded test labels \\n \",processed_test_labels[:10])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 2, 2, 2, 4, 1, 2, 3, 2, 2]\n",
            "one hot encoded test labels \n",
            "  [[0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQzOrJzG-5gc",
        "colab_type": "code",
        "outputId": "62079713-9975-4d28-9bbf-76d53f1f05e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model2.evaluate(processed_test_data,processed_test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 0s 106us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.935906858776891, 0.7081395362698755]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab 11 Dialogue Act Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_ZORURKg-fp"
      },
      "source": [
        "# Lab 11: Dialogue Act Tagging\n",
        "\n",
        "Dialogue act (DA) tagging is an important step in the process of developing dialog systems. DA tagging is a problem usually solved by supervised machine learning approaches that all require large amounts of hand labeled data. A wide range of techniques have been investigated for DA tagging. In this lab, we explore two approaches to DA classification. We are using the Switchboard Dialog Act Corpus for training.\n",
        "Corpus can be downloaded from http://compprag.christopherpotts.net/swda.html.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ziKyA9R4gyw9"
      },
      "source": [
        "The downloaded dataset should be kept in a data folder in the same directory as this file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmTpKt_uefe5",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnXMXwR0WCJQ",
        "colab_type": "code",
        "outputId": "4bf3e2bd-3e0e-4243-f463-0af86743db82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls (\"/content/drive\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `\"/content/drive\"'\n",
            "/bin/bash: -c: line 0: `ls (\"/content/drive\")'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6E8axaw1hAbM",
        "colab": {}
      },
      "source": [
        "f = glob.glob(\"/content/drive/My Drive/swda/sw*/sw*.csv\")\n",
        "frames = []\n",
        "for i in range(0, len(f)):\n",
        "    frames.append(pd.read_csv(f[i]))\n",
        "\n",
        "result = pd.concat(frames, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b7hKGF7EhM4s",
        "outputId": "5d1b1199-7057-4494-9ef0-25d262da218e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Number of converations in the dataset:\",len(result))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of converations in the dataset: 223606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ttyB2lQhc7B"
      },
      "source": [
        "The dataset has many different features, we are only using act_tag and text for this training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-jUifIdshhD0",
        "colab": {}
      },
      "source": [
        "reduced_df = result[['act_tag','text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-iPmZvysqg2i"
      },
      "source": [
        "Reduce down the number of tags to 43 - converting the combined tags to their generic classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MQuHm0jPt_lz",
        "colab": {}
      },
      "source": [
        "# Imported from \"https://github.com/cgpotts/swda\"\n",
        "# Convert the combination tags to the generic 43 tags\n",
        "\n",
        "import re\n",
        "def damsl_act_tag(input):\n",
        "        \"\"\"\n",
        "        Seeks to duplicate the tag simplification described at the\n",
        "        Coders' Manual: http://www.stanford.edu/~jurafsky/ws97/manual.august1.html\n",
        "        \"\"\"\n",
        "        d_tags = []\n",
        "        tags = re.split(r\"\\s*[,;]\\s*\", input)\n",
        "        for tag in tags:\n",
        "            if tag in ('qy^d', 'qw^d', 'b^m'): pass\n",
        "            elif tag == 'nn^e': tag = 'ng'\n",
        "            elif tag == 'ny^e': tag = 'na'\n",
        "            else: \n",
        "                tag = re.sub(r'(.)\\^.*', r'\\1', tag)\n",
        "                tag = re.sub(r'[\\(\\)@*]', '', tag)            \n",
        "                if tag in ('qr', 'qy'):                         tag = 'qy'\n",
        "                elif tag in ('fe', 'ba'):                       tag = 'ba'\n",
        "                elif tag in ('oo', 'co', 'cc'):                 tag = 'oo_co_cc'\n",
        "                elif tag in ('fx', 'sv'):                       tag = 'sv'\n",
        "                elif tag in ('aap', 'am'):                      tag = 'aap_am'\n",
        "                elif tag in ('arp', 'nd'):                      tag = 'arp_nd'\n",
        "                elif tag in ('fo', 'o', 'fw', '\"', 'by', 'bc'): tag = 'fo_o_fw_\"_by_bc'            \n",
        "            d_tags.append(tag)\n",
        "        # Dan J says (p.c.) that it makes sense to take the first;\n",
        "        # there are only a handful of examples with 2 tags here.\n",
        "        return d_tags[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S8N_PUCAblq3",
        "outputId": "e436fb0f-0d50-43f8-e2da-c8a226674a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "reduced_df[\"act_tag\"] = reduced_df[\"act_tag\"].apply(lambda x: damsl_act_tag(x))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0UNy0vvhhqpD"
      },
      "source": [
        "There are 43 tags in this dataset. Some of the tags are Yes-No-Question('qy'), Statement-non-opinion('sd') and Statement-opinion('sv'). Tags information can be found here http://compprag.christopherpotts.net/swda.html#tags. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9biiyP8UiGDe"
      },
      "source": [
        "To get unique tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BrhW8gyLfQQK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "3f8b9edf-c363-48c7-d19d-ae301e966da5"
      },
      "source": [
        "unique_tags = set()\n",
        "for tag in reduced_df['act_tag']:\n",
        "    unique_tags.add(tag)\n",
        "\n",
        "count = 1\n",
        "for tag in unique_tags:\n",
        "  print(str(count) + \"-\" + tag)\n",
        "  count += 1"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-aa\n",
            "2-sd\n",
            "3-ar\n",
            "4-qo\n",
            "5-^q\n",
            "6-x\n",
            "7-fo_o_fw_\"_by_bc\n",
            "8-qw^d\n",
            "9-qh\n",
            "10-b^m\n",
            "11-ad\n",
            "12-ba\n",
            "13-t1\n",
            "14-oo_co_cc\n",
            "15-b\n",
            "16-nn\n",
            "17-na\n",
            "18-h\n",
            "19-sv\n",
            "20-ft\n",
            "21-bf\n",
            "22-%\n",
            "23-+\n",
            "24-bd\n",
            "25-^h\n",
            "26-ny\n",
            "27-fp\n",
            "28-qy^d\n",
            "29-fa\n",
            "30-aap_am\n",
            "31-bk\n",
            "32-no\n",
            "33-t3\n",
            "34-br\n",
            "35-bh\n",
            "36-qw\n",
            "37-^g\n",
            "38-^2\n",
            "39-ng\n",
            "40-qy\n",
            "41-arp_nd\n",
            "42-fc\n",
            "43-qrr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LMOX5KwgiPmu",
        "colab": {}
      },
      "source": [
        "one_hot_encoding_dic = pd.get_dummies(list(unique_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZPHPCxE3iPby",
        "colab": {}
      },
      "source": [
        "tags_encoding = []\n",
        "for i in range(0, len(reduced_df)):\n",
        "    tags_encoding.append(one_hot_encoding_dic[reduced_df['act_tag'].iloc[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LVI8QyVzjqWh"
      },
      "source": [
        "The tags are one hot encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SQJTiffPjUtu"
      },
      "source": [
        "To create sentence embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PmkyD1TfjWGO",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "for i in range(0, len(reduced_df)):\n",
        "    sentences.append(reduced_df['text'].iloc[i].split(\" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MlD6L6e3jV-7",
        "colab": {}
      },
      "source": [
        "wordvectors = {}\n",
        "index = 1\n",
        "for s in sentences:\n",
        "    for w in s:\n",
        "        if w not in wordvectors:\n",
        "            wordvectors[w] = index\n",
        "            index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e7_cjDHrjV1c",
        "colab": {}
      },
      "source": [
        "# Max length of 137\n",
        "MAX_LENGTH = len(max(sentences, key=len))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LX6DidEvjVWs",
        "colab": {}
      },
      "source": [
        "sentence_embeddings = []\n",
        "for s in sentences:\n",
        "    sentence_emb = []\n",
        "    for w in s:\n",
        "        sentence_emb.append(wordvectors[w])\n",
        "    sentence_embeddings.append(sentence_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nr4iEyNTjmlu"
      },
      "source": [
        "Then we split the dataset into test and train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GiNZ-iI_jnOF",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, np.array(tags_encoding))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_RqMeWe_jron"
      },
      "source": [
        "And pad the sentences with zero to make all sentences of equal length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yqD7DvzRGRY7",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 137"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ai9cwv82jufe",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "517zYSQLXkbn",
        "colab": {}
      },
      "source": [
        "train_input = train_sentences_X[:140000]\n",
        "val_input = train_sentences_X[140000:]\n",
        "\n",
        "train_labels = y_train[:140000]\n",
        "val_labels = y_train[140000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHJbZDtk7N-3"
      },
      "source": [
        "# Model 1 - \n",
        "\n",
        "The first approach we'll try is to treat DA tagging as a standard multi-class text classification task, in the way you've done before with sentiment analysis and other tasks. Each utterance will be treated independently as a text to be classified with its DA tag label. This model has an architecture of:\n",
        "\n",
        "- Embedding  \n",
        "- BLSTM  \n",
        "- Fully Connected Layer\n",
        "- Softmax Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FItlHC1Fjz6y"
      },
      "source": [
        " The model architecture is as follows: Embedding Layer (to generate word embeddings) Next layer Bidirectional LSTM. Feed forward layer with number of neurons = number of tags. Softmax activation to get the probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M97Sw5iv-lEU",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(wordvectors) # 43,731\n",
        "MAX_LENGTH = len(max(sentences, key=len))\n",
        "EMBED_SIZE = 100 # arbitary\n",
        "HIDDEN_SIZE = len(unique_tags) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCaX-ptaj8G2",
        "outputId": "c46ab213-3b65-4b44-81f1-437d8249db29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#Building the network\n",
        "model = Sequential()\n",
        "# Include 2 BLSTM layers, in order to capture both the forward and backward hidden states\n",
        "# Embedding layer\n",
        "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length = MAX_LENGTH))\n",
        "# Bidirectional 1\n",
        "model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))\n",
        "# Bidirectional 2\n",
        "model.add(Bidirectional(LSTM(HIDDEN_SIZE)))\n",
        "# Dense layer\n",
        "model.add(Dense(HIDDEN_SIZE))\n",
        "# Activation\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 137, 100)          4373100   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 137, 86)           49536     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 86)                44720     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 43)                3741      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 4,471,097\n",
            "Trainable params: 4,471,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeiLkgD3Arpl",
        "outputId": "fedc035f-62c4-4e78-8caf-b453c8af90df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Train the model - using validation \n",
        "model.fit(train_input, train_labels, batch_size=512, epochs=3, validation_data=(val_input, val_labels))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 140000 samples, validate on 27704 samples\n",
            "Epoch 1/3\n",
            "140000/140000 [==============================] - 358s 3ms/step - loss: 1.7517 - accuracy: 0.5291 - val_loss: 1.3637 - val_accuracy: 0.6187\n",
            "Epoch 2/3\n",
            "140000/140000 [==============================] - 350s 3ms/step - loss: 1.2108 - accuracy: 0.6516 - val_loss: 1.1408 - val_accuracy: 0.6674\n",
            "Epoch 3/3\n",
            "140000/140000 [==============================] - 349s 2ms/step - loss: 1.0309 - accuracy: 0.7032 - val_loss: 1.0749 - val_accuracy: 0.6833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2f5eb90550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2LkONUKQkSrL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e77bdca-7214-46aa-e066-598640f063e8"
      },
      "source": [
        "score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55902/55902 [==============================] - 3s 52us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ab0ZL1dqkTY4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a1dea4b-cf6e-4b39-feab-708ce5afe6f0"
      },
      "source": [
        "print(\"Overall Accuracy:\", score[1]*100)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall Accuracy: 70.46080827713013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LhMViQVSPY1J"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "\n",
        "The overall accuracy is 67%, an effective accuracy for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHwoVCEwjEz7"
      },
      "source": [
        "In addition to overall accuracy, you need to look at the accuracy of some minority classes. Signal-non-understanding ('br') is a good indicator of \"other-repair\" or cases in which the other conversational participant attempts to repair the speaker's error. Summarize/reformulate ('bf') has been used in dialogue summarization. Report the accuracy for these classes and some frequent errors you notice the system makes in predicting them. What do you think the reasons are？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7owA1f27se8"
      },
      "source": [
        "## Minority Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZ8BwgDxNcIr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "42fbc7fd-34b7-473a-eaaa-102b75ca1ea8"
      },
      "source": [
        "# Generate predictions for the test data\n",
        "generate_test_pred = model.predict(test_sentences_X, batch_size=100)\n",
        "print(generate_test_pred)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.2740885e-04 6.0133183e-01 6.3432424e-05 ... 6.0046732e-04\n",
            "  6.4304858e-02 2.8837247e-05]\n",
            " [1.5235813e-01 1.3452207e-04 3.0221130e-05 ... 1.2298423e-04\n",
            "  1.0983916e-03 3.4793328e-07]\n",
            " [4.1430299e-05 9.3947524e-01 4.3663595e-06 ... 6.7220964e-05\n",
            "  1.1845243e-04 1.1921462e-05]\n",
            " ...\n",
            " [1.3241613e-02 5.5170993e-05 5.0603976e-06 ... 4.6123387e-05\n",
            "  2.5910011e-03 1.7073411e-06]\n",
            " [3.0896487e-03 2.5652664e-05 8.5570878e-07 ... 6.4598726e-06\n",
            "  9.6556137e-04 1.0871341e-06]\n",
            " [5.1337037e-02 3.8717888e-05 1.0910450e-05 ... 8.9077090e-05\n",
            "  1.7908460e-03 1.9788281e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5I26g20qQdzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "35613d52-1958-4b6e-a992-6d6409ef167f"
      },
      "source": [
        "# Build the confusion matrix off these predictions\n",
        "matrix_balanced = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), generate_test_pred.argmax(axis=1))\n",
        "print(matrix_balanced)\n",
        "\n",
        "matrix_balanced = matrix_balanced.astype('float')/matrix_balanced.sum(axis=1)[:,np.newaxis]\n",
        "matrix_balanced= matrix_balanced.diagonal()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  782   135     0 ...     0     9     0]\n",
            " [  115 16066     1 ...     0    34     7]\n",
            " [    2    16     0 ...     0     0     0]\n",
            " ...\n",
            " [    0    30     0 ...     0     0     0]\n",
            " [   12    80     0 ...     0   386     2]\n",
            " [    0     5     0 ...     0     0    34]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "muWtF2t0W0zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0cc05d3-3378-41e1-e294-45c806cefaa9"
      },
      "source": [
        "# Calculate Accuracies for \"br\" and \"bf\"\n",
        "# sklearn.metrics.accuracy_score(y_test.argmax(axis = 1), generate_test_pred.argmax(axis = 1), normalize=True, sample_weight=None)\n",
        "bf_accuracy = matrix_balanced[20]\n",
        "br_accuracy = matrix_balanced[33]\n",
        "\n",
        "print(bf_accuracy)\n",
        "print(br_accuracy)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.05627705627705628\n",
            "0.5342465753424658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HdnpWLggZ-6z"
      },
      "source": [
        "\n",
        "Due to the reduced lack of training data for the minority classes, these minority classifiers will not be very confident in classification, as they have not been fully optimised. The frequent classifiers will be more optimised and will generate more confident scores for all examples, effectively crowding out the less confident minority classifiers. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZ16sE5F7x9e"
      },
      "source": [
        "# Model 2 - Balanced Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKHbOs4WkFaP"
      },
      "source": [
        "\n",
        "One thing we can do to try to improve performance is therefore to balance the data more sensibly. As the dataset is highly imbalanced, we can simply weight up the minority classes proportionally to their underrepresentation while training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6L4kNdf6kGEa",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "y_integers = np.argmax(tags_encoding, axis=1)\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "d_class_weights = dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF1UM-ZMZoa1"
      },
      "source": [
        "## Define & Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xIRgRAzOPSAZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "93cd3787-81d8-4e6f-e1a9-36c9182e72e6"
      },
      "source": [
        "# Re-built the model for the balanced training\n",
        "#Building the network\n",
        "balanced_model = Sequential()\n",
        "# Include 2 BLSTM layers, in order to capture both the forward and backward hidden states\n",
        "# Embedding layer\n",
        "balanced_model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length = MAX_LENGTH))\n",
        "# Bidirectional 1\n",
        "balanced_model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))\n",
        "# Bidirectional 2\n",
        "balanced_model.add(Bidirectional(LSTM(HIDDEN_SIZE)))\n",
        "# Dense layer\n",
        "balanced_model.add(Dense(HIDDEN_SIZE))\n",
        "# Activation\n",
        "balanced_model.add(Activation('softmax'))\n",
        "balanced_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "balanced_model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 137, 100)          4373100   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 137, 86)           49536     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 86)                44720     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 43)                3741      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 4,471,097\n",
            "Trainable params: 4,471,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xB2McUREkL4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6027a41e-0764-43ba-ae41-e1b62de137a9"
      },
      "source": [
        "# Train the balanced network -  takes  time to achieve good accuracy\n",
        "balanced_model.fit(train_input, train_labels, batch_size=512, epochs=3, validation_data=(val_input, val_labels))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 140000 samples, validate on 27704 samples\n",
            "Epoch 1/3\n",
            "140000/140000 [==============================] - 356s 3ms/step - loss: 1.7512 - accuracy: 0.5272 - val_loss: 1.3435 - val_accuracy: 0.6221\n",
            "Epoch 2/3\n",
            "140000/140000 [==============================] - 355s 3ms/step - loss: 1.2204 - accuracy: 0.6528 - val_loss: 1.1610 - val_accuracy: 0.6604\n",
            "Epoch 3/3\n",
            "140000/140000 [==============================] - 356s 3ms/step - loss: 1.0207 - accuracy: 0.7078 - val_loss: 1.0607 - val_accuracy: 0.6889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2f5e0fbcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DJPjlMclZtw2"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8UMAMGpJRINC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a8d4b46-e06b-4514-f080-1432eb0f6da1"
      },
      "source": [
        "# Overall Accuracy\n",
        "score = balanced_model.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55902/55902 [==============================] - 52s 929us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0xzLIkTarjei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac68eea8-d14d-487b-bccd-1b2cbbddd790"
      },
      "source": [
        "print(\"Overall Accuracy:\", score[1]*100)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall Accuracy: 69.16747093200684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qkULcz2igEW3",
        "colab": {}
      },
      "source": [
        "# Generate predictions for the test data\n",
        "label_pred = balanced_model.predict(test_sentences_X, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hq7i7giWZ4_l"
      },
      "source": [
        "## Balanced network evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fM7VWweco0Et"
      },
      "source": [
        "Report the overall accuracy and the accuracy of  'br' and 'bf'  classes. Suggest other ways to handle imbalanced classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jNfWmSNgRvT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73431d78-c849-41e2-d55d-b5b953fc94cc"
      },
      "source": [
        "# Build the confusion matrix off these predictions\n",
        "matrix_balanced = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1))\n",
        "\n",
        "# Calculate Accuracies for \"br\" and \"bf\"\n",
        "sklearn.metrics.accuracy_score(y_test.argmax(axis = 1), generate_test_pred.argmax(axis = 1), normalize=True, sample_weight=None)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6827841579907695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zi9GyVUvPcrF"
      },
      "source": [
        "\n",
        "\n",
        "### Accuracies\n",
        "\n",
        "\n",
        "\n",
        "### Explanation\n",
        "\n",
        "\n",
        "### Other ways to handle imbalanced classes\n",
        "\n",
        "\n",
        "- \n",
        "\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fW4g5mQkkaFv"
      },
      "source": [
        "Can we improve things by using context information?  Next we try to build a model which predicts DA tag from the sequence of \n",
        "previous DA tags, plus the utterance representation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WfrGWuZ6nk4y"
      },
      "source": [
        "# Using Context for Dialog Act Classification\n",
        "\n",
        "The second approach we will try is a hierarchical approach to DA tagging. We expect there is valuable sequential information among the DA tags. So in this section we apply a BiLSTM on top of the sentence CNN representation. The CNN model learns textual information in each utterance for DA classification, acting like the text classifier from Model 1 above. Then we use a bidirectional-LSTM (BLSTM) above that to learn how to use the context before and after the current utterance to improve the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7qyPpNaK-2mb"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "This model has an architecture of:\n",
        "\n",
        "- Word Embedding\n",
        "- CNN\n",
        "- Bidirectional LSTM\n",
        "- Fully-Connected output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DuJLqgjWqcIf"
      },
      "source": [
        "## CNN\n",
        "\n",
        "\n",
        "This is a classical CNN layer used to convolve over embedings tensor and gether useful information from it. The data is represented by hierarchy of features, which can be modelled using a CNN. We transform/reshape conv output to 2d matrix. Then we pass it to the max pooling layer that applies the max pool operation on windows of different sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA5INtFl-fM0",
        "colab": {}
      },
      "source": [
        "filter_sizes = [3,4,5]\n",
        "num_filters = 64\n",
        "drop = 0.2\n",
        "VOCAB_SIZE = len(wordvectors) # 43,731\n",
        "MAX_LENGTH = len(max(sentences, key=len))\n",
        "EMBED_SIZE = 100 # arbitary\n",
        "HIDDEN_SIZE = len(unique_tags) \n",
        "\n",
        "from keras.layers import Conv2D,BatchNormalization,MaxPool2D,Concatenate,Reshape,Flatten, Input\n",
        "\n",
        "# CNN model\n",
        "inputs = Input(shape=(MAX_LENGTH, ), dtype='int32')\n",
        "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_LENGTH)(inputs)\n",
        "reshape = Reshape((MAX_LENGTH, EMBED_SIZE, 1))(embedding)\n",
        "\n",
        "# 3 convolutions\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "bn_0 = BatchNormalization()(conv_0)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "bn_1 = BatchNormalization()(conv_1)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "bn_2 = BatchNormalization()(conv_2)\n",
        "\n",
        "# maxpool for 3 layers\n",
        "maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), padding='valid')(bn_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), padding='valid')(bn_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), padding='valid')(bn_2)\n",
        "\n",
        "# concatenate tensors\n",
        "concatenation = Concatenate( axis = -1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "# flatten concatenated tensors\n",
        "flatten = Flatten() (concatenation)\n",
        "# dense layer (dense_1)\n",
        "dense_1 = Dense(HIDDEN_SIZE) (flatten)\n",
        "# dropout_1\n",
        "dropout_1 = Dropout(drop)(dense_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wDuERMw7-rAV"
      },
      "source": [
        "## BLSTM\n",
        "\n",
        "This is used to create LSTM layers. The data we’re working with has temporal properties which we want to model as well — hence the use of a LSTM. You should create a BiLSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pFGp2EWI-fM7",
        "colab": {}
      },
      "source": [
        "# BLSTM model\n",
        "timedist = TimeDistributed(Flatten()) (concatenation)\n",
        "# Bidirectional 1\n",
        "bidirection1 = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences = True)) (timedist)\n",
        "# Bidirectional 2\n",
        "bidirection2 = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences = False)) (bidirection1)\n",
        "# Dense layer (dense_2)\n",
        "dense_2 = Dense(HIDDEN_SIZE) (bidirection2)\n",
        "# dropout_2\n",
        "dropout_2 = Dropout(drop)(dense_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7wluAkx6AQUb"
      },
      "source": [
        "Concatenate 2 last layers and create the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kzrhgkX2-fNE",
        "colab": {}
      },
      "source": [
        "from  keras import Model\n",
        "# concatenate 2 final layers\n",
        "concatenation2 = Concatenate( axis = -1)([dropout_1, dropout_2])\n",
        "# output\n",
        "outputlayer = Dense(HIDDEN_SIZE, activation = 'softmax') (concatenation2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Jneg-GD-fNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "8d7f6353-7014-4abf-f1ee-d4fb65b37541"
      },
      "source": [
        "# Train the model - using validation\n",
        "model = Model(inputs, outputlayer)\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_input, train_labels, batch_size=100, epochs=3, validation_data=(val_input, val_labels))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 140000 samples, validate on 27704 samples\n",
            "Epoch 1/3\n",
            "140000/140000 [==============================] - 54s 389us/step - loss: 1.0821 - accuracy: 0.6759 - val_loss: 0.9082 - val_accuracy: 0.7143\n",
            "Epoch 2/3\n",
            "140000/140000 [==============================] - 48s 343us/step - loss: 0.7943 - accuracy: 0.7512 - val_loss: 0.9224 - val_accuracy: 0.7114\n",
            "Epoch 3/3\n",
            "140000/140000 [==============================] - 48s 341us/step - loss: 0.6404 - accuracy: 0.7964 - val_loss: 1.0080 - val_accuracy: 0.7058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2f523abac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MSMRSX1u-fNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b10de3b-34c6-4fd7-c9a2-83fbfff97085"
      },
      "source": [
        "score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55902/55902 [==============================] - 3s 52us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3qFMsXNS-fNS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8587d209-dc27-4dfa-e7f3-c7bb358fe6a6"
      },
      "source": [
        "print(\"Overall Accuracy:\", score[1]*100)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall Accuracy: 70.46080827713013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RKMmrfuisKGJ"
      },
      "source": [
        "Report your overall accuracy. Did context help disambiguate and better predict the minority classes ('br' and 'bf')? What are frequent errors? Show one positive example where adding context changed the prediction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QmO6hVsWTaNr"
      },
      "source": [
        "### Minority Classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gUZt48JgrE34"
      },
      "source": [
        "# Advanced:  Bert-Based Model for Dialogue Act Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE63Q5guuPdA"
      },
      "source": [
        "In the last section we want to use BERT and leverage contextual word embeddings, following on from the last lab you've \n",
        "just done. This is an advanced part of the assignment and worth 10 marks (20%) in total. You could use your BERT-based text classifier here (instead of the CNN utterance-level classifier) and see if a pre-trained BERT language model helps. The domain difference from conversational data is one possible downside to using BERT. Explore some techniques to efficiently transfer the knowledge from conversational data and to improve model performance on DA tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MwTHV4Q93Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}